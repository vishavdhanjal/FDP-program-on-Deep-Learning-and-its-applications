{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HandsOn_RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"omnexuMiXO_t"},"source":["## Import Basic Function - (Basic Python and Pytorch)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NXcEd1u688TU","colab":{}},"source":["# import and other functions\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.optim as optim\n","\n","from typing import *\n","from pathlib import Path\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ImM8c0-u8iUc"},"source":["## Downloading dataset "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nZTA8uOz9BrD","outputId":"8e946342-c7e3-4562-e51f-8585513367e0","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1576133370942,"user_tz":0,"elapsed":6195,"user":{"displayName":"Ritu Ahluwalia","photoUrl":"","userId":"04192045249850159450"}}},"source":["# code to datset download\n","DATA_ROOT = Path(\"../data/brown\")\n","!mkdir -p {DATA_ROOT}\n","# We'll be using the Brown Corpus which you can get via the commands below.\n","!curl http://www.sls.hawaii.edu/bley-vroman/brown.txt -o {DATA_ROOT / \"brown.txt\"}"],"execution_count":5,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 6040k  100 6040k    0     0  4741k      0  0:00:01  0:00:01 --:--:-- 4741k\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rU_MHa889KR8"},"source":["# Simple RNN (hand composed)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-45Ah4K20nmE"},"source":["we'll be building our own RNN and delving into why it performs so well across a wide range of tasks."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8Yn_Q4JJ4g-t"},"source":["Lets recall the update rule for RNN \n","$$ c_t = \\tanh(W_hc_{t-1} + W_ix_t) $$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SEfINXzx4v90","colab":{}},"source":["class SimpleRNN(nn.Module):\n","    def __init__(self, input_sz: int, hidden_sz: int):\n","        super().__init__()\n","        self.input_sz, self.hidden_size = input_sz, hidden_sz\n","        self.weight_ih = Parameter(torch.Tensor(input_sz, hidden_sz))\n","        self.weight_hh = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n","        self.bias_hh = Parameter(torch.Tensor(hidden_sz))\n","        \n","        self.init_weights()\n","\n","    def init_weights(self):\n","        nn.init.xavier_uniform_(self.weight_ih)\n","        nn.init.xavier_uniform_(self.weight_hh)\n","        nn.init.zeros_(self.bias_hh)\n","    \n","    def forward(self, x: torch.Tensor, init_state=None) -> torch.Tensor:\n","        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n","        bs, seq_sz, _ = x.size()\n","        hidden_seq = []\n","        if init_state is None:\n","            h_t = torch.zeros(self.hidden_size).to(x.device)\n","        else:\n","            h_t = init_state\n","\n","        for t in range(seq_sz):\n","            x_t = x[:, t, :]\n","            h_t = torch.tanh(x_t @ self.weight_ih + h_t @ self.weight_hh + self.bias_hh)\n","            hidden_seq.append(h_t.unsqueeze(Dim.batch))\n","        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n","        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n","        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n","        return hidden_seq, h_t\n","        \n","from enum import IntEnum\n","class Dim(IntEnum):\n","    batch = 0\n","    seq = 1\n","    feature = 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QR4ht1Xk7Kq6"},"source":["## Execution"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jUwf3A4I47Uc"},"source":["\n","Testing on some synthetic data\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7VvTL4xS46S1","colab":{}},"source":["#code to execute simple rnn\n","bs, seq_len, feat_sz, hidden_sz = 5, 10, 32, 16\n","arr = torch.randn(bs, seq_len, feat_sz)\n","rnn = SimpleRNN(feat_sz, hidden_sz)\n","hidden_state, h_t = rnn(arr)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QGo2Zr417qDG"},"source":["#Visualization"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EBbDNOBY5Eg5","outputId":"ce5959af-ac7d-4ef3-fdcd-a5af5cefb6a3","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576133370945,"user_tz":0,"elapsed":6175,"user":{"displayName":"Ritu Ahluwalia","photoUrl":"","userId":"04192045249850159450"}}},"source":["# code to plot results\n","print(h_t.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["torch.Size([5, 16])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t5aPZ4Zi7xyK","outputId":"e2a85f1d-4128-4b07-acbe-8ce68da41e1c","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576133371297,"user_tz":0,"elapsed":6490,"user":{"displayName":"Ritu Ahluwalia","photoUrl":"","userId":"04192045249850159450"}}},"source":["print(hidden_state.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["torch.Size([5, 10, 16])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JNEGKM2A6ykN"},"source":["To validate the rnn code check  the shape of hidden_seq and h_t : torch.Size([5, 10, 16]) and torch.Size([5, 16]). "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IepDP7n959yX"},"source":["RNN seems working. We will check the performance and compare with LSTM. We will **visualize** the performance in a language model setting with character based encoding architecture for sentiment analysis."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9gPZ5sYm8ITs"},"source":[""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Sk9dfMA_9V4D"},"source":["# Simple LSTM (encoder) - using pytorch (or tensorflow)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mJ-clYy11Al7"},"source":["Before we actually build the LSTM, Recall its basic mechansim. The equation for the LSTM looks like this: \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pjd4SsCX1RNx"},"source":["\\begin{array}{ll} \\\\\n","            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n","            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n","            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n","            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n","            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n","            h_t = o_t * \\tanh(c_t) \\\\\n","        \\end{array}"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7Z_RfKbS1iOy"},"source":["It seems complex, but when you pick it apart, the LSTM is actually very simple. The core of the LSTM is the following equation:\n","\n","\\begin{array}{ll} \\\\\n","            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n","\\end{array}"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CzvjbT601s4P"},"source":["Let's pick this equation apart: $ c_t $ is the new cell state, which is basically the memory of the LSTM. \n","\n","$ f_t $ is called the \"forget gate\": it dictates how much of the previous cell state to **retain** (but is slightly confusingly named the forget gate). \n","\n","$ i_t $ is the \"input gate\" and dictates how much to update the cell state with new information.\n","\n","Finally, $ g_t $ is the information we use to update the cell state.\n","\n","Basically, an LSTM chooses to keep a certain portion of its previous cell state and add a certain amount of new information. These proportions are controlled using gates.\n","\n","Let's contrast this update rule with the update rule of a simpler RNN.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ubOV8YPI100G"},"source":["$$ c_t = \\tanh(W_hc_{t-1} + W_ix_t) $$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TPCzTidd2gS_"},"source":["(To make the contrast clearer, I'm representing the hidden state of the RNN as $ c_t $.)\n","\n","As you can see, there is a huge difference between the simple RNN's update rule and the LSTM's update rule. Whereas the RNN computes the new hidden state from scratch based on the previous hidden state and the input, the LSTM computes the new hidden state by choosing what to **add** to the current state. This is similar to how ResNets learn: they learn what to add to the current state/block instead of directly learning the new state. In other words, LSTMs are great primarily because they are **additive**. We'll formalize this intuition later when we examine the gradient flow, but this is the basic idea behind the LSTM.\n","\n","Now that we have a basic understanding, let's start coding."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"On-4UVqo21wt"},"source":["Side Note: One thing that is slightly confusing about the LSTM is that it has two \"hidden states\": $ c_t $ and $ h_t $. Intuitively, $ c_t $ is the \"internal\" hidden state that retains important information for longer timesteps, whereas $ h_t $ is the \"external\" hidden state that exposes that information to the outside world."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N4BtZTWB23HU"},"source":["Side Note: If you're looking carefully, you'll notice that the bias terms are redundant. The reason they are there is for compatibility with the CuDNN backend. Until we touch on CuDNN, we'll use a single bias term."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EgaPzsaD3Au2"},"source":["# Implementing the LSTM\n","We'll be using PyTorch to write our own LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XjwBp3di1PYg","colab":{}},"source":["from enum import IntEnum\n","class Dim(IntEnum):\n","    batch = 0\n","    seq = 1\n","    feature = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"grQMg2aA3MjR","colab":{}},"source":["class NaiveLSTM(nn.Module):\n","    def __init__(self, input_sz: int, hidden_sz: int):\n","        super().__init__()\n","        self.input_size = input_sz\n","        self.hidden_size = hidden_sz\n","        # input gate\n","        self.W_ii = Parameter(torch.Tensor(input_sz, hidden_sz))\n","        self.W_hi = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n","        self.b_i = Parameter(torch.Tensor(hidden_sz))\n","        # forget gate\n","        self.W_if = Parameter(torch.Tensor(input_sz, hidden_sz))\n","        self.W_hf = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n","        self.b_f = Parameter(torch.Tensor(hidden_sz))\n","        # ???\n","        self.W_ig = Parameter(torch.Tensor(input_sz, hidden_sz))\n","        self.W_hg = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n","        self.b_g = Parameter(torch.Tensor(hidden_sz))\n","        # output gate\n","        self.W_io = Parameter(torch.Tensor(input_sz, hidden_sz))\n","        self.W_ho = Parameter(torch.Tensor(hidden_sz, hidden_sz))\n","        self.b_o = Parameter(torch.Tensor(hidden_sz))\n","        \n","        self.init_weights()\n","    \n","    def init_weights(self):\n","        for p in self.parameters():\n","            if p.data.ndimension() >= 2:\n","                nn.init.xavier_uniform_(p.data)\n","            else:\n","                nn.init.zeros_(p.data)\n","        \n","    def forward(self, x: torch.Tensor, \n","                init_states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None\n","               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n","        bs, seq_sz, _ = x.size()\n","        hidden_seq = []\n","        if init_states is None:\n","            h_t, c_t = torch.zeros(self.hidden_size).to(x.device), torch.zeros(self.hidden_size).to(x.device)\n","        else:\n","            h_t, c_t = init_states\n","        for t in range(seq_sz): # iterate over the time steps\n","            x_t = x[:, t, :]\n","            i_t = torch.sigmoid(x_t @ self.W_ii + h_t @ self.W_hi + self.b_i)\n","            f_t = torch.sigmoid(x_t @ self.W_if + h_t @ self.W_hf + self.b_f)\n","            g_t = torch.tanh(x_t @ self.W_ig + h_t @ self.W_hg + self.b_g)\n","            o_t = torch.sigmoid(x_t @ self.W_io + h_t @ self.W_ho + self.b_o)\n","            c_t = f_t * c_t + i_t * g_t\n","            h_t = o_t * torch.tanh(c_t)\n","            hidden_seq.append(h_t.unsqueeze(Dim.batch))\n","        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n","        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n","        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n","        return hidden_seq, (h_t, c_t)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VrVm66Zt8oYI"},"source":["## Execution"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XO-JI5IT3ahB"},"source":["Testing on some synthetic data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ybzg1WKl3ZfN","colab":{}},"source":["# code to execute simple lstm (encoder)\n","bs, seq_len, feat_sz, hidden_sz = 5, 10, 32, 16\n","arr = torch.randn(bs, seq_len, feat_sz)\n","lstm = NaiveLSTM(feat_sz, hidden_sz)\n","hs, (hn, cn) = lstm(arr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K_uIuyWI3hav","outputId":"1e828ab7-e3f8-4078-b46f-830297e3b33e","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576133371299,"user_tz":0,"elapsed":6464,"user":{"displayName":"Ritu Ahluwalia","photoUrl":"","userId":"04192045249850159450"}}},"source":["hs.shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 10, 16])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HwAMLda93rWN"},"source":["To  validate the LSTM code check the o/p of hs size   : \n","\"torch.Size([5, 10, 16])\"\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tdY5nJ24_IcT"},"source":["## Visualization"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4oY8LfyG-Ui5"},"source":["## Testing our implementation\n","Now, that we've covered the basics and have a minimally working RNN and LSTM, we'll put our model into action. Our testbed will be a character-level language modeling task. We'll be using the Brown Corpus."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cEJ3q5dD-o8k","outputId":"fbe10612-b8ca-4f10-caa9-f87bad2b0dbe","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1576133455756,"user_tz":0,"elapsed":90890,"user":{"displayName":"Ritu Ahluwalia","photoUrl":"","userId":"04192045249850159450"}}},"source":["! pip install allennlp"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 3.3MB/s \n","\u001b[?25hCollecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n","\u001b[K     |████████████████████████████████| 194kB 42.1MB/s \n","\u001b[?25hCollecting pytorch-transformers==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 21.7MB/s \n","\u001b[?25hCollecting numpydoc>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n","Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n","Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n","Collecting flask-cors>=3.0.7\n","  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 40.6MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.4)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n","Collecting word2number>=1.1\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Collecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Collecting pytorch-pretrained-bert>=0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 45.6MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n","Collecting parsimonious>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n","\u001b[?25hCollecting overrides\n","  Downloading https://files.pythonhosted.org/packages/8b/a0/0d4a9dcd28de9bf263e9436b5876ba199cc4e4a19d0dc4e85eed657b1b77/overrides-2.5.tar.gz\n","Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n","\u001b[K     |████████████████████████████████| 256kB 46.0MB/s \n","\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n","Collecting conllu==1.3.1\n","  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n","Collecting responses>=0.7\n","  Downloading https://files.pythonhosted.org/packages/6e/e1/a078cbbbe088d1099ea73a6bb4cd08a10ec9ab35153eed0d86b8f3802ee8/responses-0.10.7-py2.py3-none-any.whl\n","Collecting flaky\n","  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.32)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n","Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.3)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.12.0)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 34.8MB/s \n","\u001b[?25hCollecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/db/4b29a0adec5881542cd81cb5d1929b5c0787003c5740b3c921e627d9c2e5/regex-2019.12.9.tar.gz (669kB)\n","\u001b[K     |████████████████████████████████| 675kB 36.9MB/s \n","\u001b[?25hRequirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n","Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.32)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.4.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n","Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n","Building wheels for collected packages: numpydoc, word2number, ftfy, parsimonious, overrides, jsonnet, regex\n","  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for numpydoc: filename=numpydoc-0.9.1-cp36-none-any.whl size=31872 sha256=1ff259229df8ea128c72229bd16b332ed5c446f4a03c2d015c9a78e4cb60cb92\n","  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=704f7e60eba0d1b4eb514adacfb365c98d0dbf9bb9d29531cbd92dbcd9bed02b\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=cb2a40de74e488a1378b4ca624d2eee6c8cadd961637894004b467901cac6631\n","  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=0be98f365d5b998bb1310354064947dc66c87327744581be740a7d7143cefa94\n","  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.5-cp36-none-any.whl size=5407 sha256=3f20714fcd5ab526b6ba6b8877c41ac8fd1e13994525ab9d41f79777539a34e0\n","  Stored in directory: /root/.cache/pip/wheels/be/56/0c/f5ce8e2403ae7d881e5d638fca1c1c5d21dac8eb6df5a6950f\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320359 sha256=3d4f4ddfeb556a3581ffc80e58559698f2cddfd14a6f58118c4b7a73447daa75\n","  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.12.9-cp36-cp36m-linux_x86_64.whl size=609189 sha256=e5eb1db4c3bb1051e6aa965614eae8e46f15d24f2a0a51514aa6a4f3f6cfbd85\n","  Stored in directory: /root/.cache/pip/wheels/0d/fb/b3/a89169557229468c49ca64f6839418f22461f6ee0a74f342b1\n","Successfully built numpydoc word2number ftfy parsimonious overrides jsonnet regex\n","Installing collected packages: tensorboardX, sentencepiece, regex, pytorch-transformers, numpydoc, flask-cors, unidecode, word2number, ftfy, pytorch-pretrained-bert, parsimonious, overrides, jsonnet, conllu, responses, flaky, jsonpickle, allennlp\n","Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-2.5 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 regex-2019.12.9 responses-0.10.7 sentencepiece-0.1.83 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l941M2m5-uDU"},"source":["We'll let AllenNLP handle the complexity of training the language model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9vRFvodb-2c8","outputId":"5a816200-dfb2-472b-9633-298be2afc381","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1576133479060,"user_tz":0,"elapsed":114184,"user":{"displayName":"Ritu Ahluwalia","photoUrl":"","userId":"04192045249850159450"}}},"source":["from allennlp.data.dataset_readers import LanguageModelingReader\n","from allennlp.data.tokenizers import CharacterTokenizer\n","from allennlp.data.token_indexers import SingleIdTokenIndexer\n","from allennlp.data import Vocabulary\n","from allennlp.data.iterators import BasicIterator\n","from allennlp.training import Trainer\n","from sklearn.model_selection import train_test_split\n","\n","char_tokenizer = CharacterTokenizer(lowercase_characters=True)\n","\n","reader = LanguageModelingReader(\n","    tokens_per_instance=500,\n","    tokenizer=char_tokenizer,\n","    token_indexers = {\"tokens\": SingleIdTokenIndexer()},\n",")\n","\n","train_ds = reader.read(DATA_ROOT / \"brown.txt\")\n","train_ds, val_ds = train_test_split(train_ds, random_state=0, test_size=0.1)\n","\n","vocab = Vocabulary.from_instances(train_ds)\n","\n","iterator = BasicIterator(batch_size=32)\n","iterator.index_with(vocab)\n","\n","def train(model: nn.Module, epochs: int=10):\n","    trainer = Trainer(\n","        model=model.cuda() if torch.cuda.is_available() else model,\n","        optimizer=optim.Adam(model.parameters()),\n","        iterator=iterator, train_dataset=train_ds, \n","        validation_dataset=val_ds, num_epochs=epochs,\n","        cuda_device=0 if torch.cuda.is_available() else -1\n","    )\n","    return trainer.train()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0it [00:00, ?it/s]\n","  0%|          | 0/11994 [00:00<?, ?it/s]\u001b[A\n"," 31%|███       | 3711/11994 [00:00<00:00, 23895.66it/s]\u001b[A\n"," 66%|██████▌   | 7902/11994 [00:00<00:00, 27359.17it/s]\u001b[A\n","11994it [00:13, 914.90it/s]\n","100%|██████████| 10794/10794 [00:05<00:00, 1823.01it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UVp3rXiF-5bN","colab":{}},"source":["from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","from allennlp.models import Model\n","from allennlp.nn.util import get_text_field_mask\n","\n","class LanguageModel(Model):\n","    def __init__(self, encoder: nn.RNN, vocab: Vocabulary,\n","                 embedding_dim: int=50):\n","        super().__init__(vocab=vocab)\n","        # char embedding\n","        self.vocab_size = vocab.get_vocab_size()\n","        self.padding_idx = vocab.get_token_index(\"@@PADDING@@\")\n","        token_embedding = Embedding(\n","            num_embeddings=vocab.get_vocab_size(),\n","            embedding_dim=embedding_dim,\n","            padding_index=self.padding_idx,\n","        )\n","        self.embedding = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n","        self.encoder = encoder\n","        self.projection = nn.Linear(self.encoder.hidden_size, self.vocab_size)\n","        self.loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx)\n","    \n","    def forward(self, input_tokens: Dict[str, torch.Tensor],\n","                output_tokens: Dict[str, torch.Tensor]):\n","        # TODO: Implement\n","        embs = self.embedding(input_tokens)\n","        x, _ = self.encoder(embs)\n","        x = self.projection(x)\n","        if output_tokens is not None:\n","            loss = self.loss(x.view((-1, self.vocab_size)), output_tokens[\"tokens\"].flatten())\n","        else:\n","            loss = None\n","        return {\"loss\": loss, \"logits\": x}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jSJn8JV0_FbA"},"source":["Now, let's try training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZlCE8e42_CGi","outputId":"da2e381e-8691-48de-a571-f0d6dc02aad7","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["N_EPOCHS = 1\n","lm_naive = LanguageModel(NaiveLSTM(50, 125), vocab)\n","train(lm_naive, epochs=N_EPOCHS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n","loss: 2.6683 ||: 100%|██████████| 338/338 [06:07<00:00,  1.08s/it]\n","loss: 2.3115 ||:  71%|███████   | 27/38 [00:09<00:03,  2.93it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ePxFLbTe_fkW"},"source":["Now, let's compare with the official LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CtbCpHXo_a__","colab":{}},"source":["lm_comparison = LanguageModel(nn.LSTM(50, 125, batch_first=True), vocab)\n","train(lm_comparison, epochs=N_EPOCHS)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KZEYP1dt_jh9"},"source":["Out model is a lot slower, but we're getting similar performance, so it looks good! We'll look at how we can optimize the performance later.\n","\n","Now, let's compare the performance of the LSTM with a much simpler RNN"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FrKBBPh5_sKt","colab":{}},"source":["lm_simplernn = LanguageModel(SimpleRNN(50, 125), vocab)\n","train(lm_simplernn, epochs=N_EPOCHS)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yoih9JeJ_yh8"},"source":["# Understanding the dynamics of LSTM learning"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gSBQDKoL_2i1"},"source":["Why exactly do LSTMs learn so well? Let's analyze the dynamics of LSTM learning by checking how the gradients change and comparing them to the gradients of a simple RNN."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zGBKtSXD_4yI","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","test_batch = next(iterator(train_ds))\n","test_embeddings = lm_naive.embedding(test_batch[\"input_tokens\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nd1AV6pU_9Xx"},"source":["### The gradient dynamics of simple RNNs\n","\n","First, let's check how the gradients of a simple RNN change with regards to the initial inputs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uoDTkcOWAFYB","colab":{}},"source":["\n","def rnn_step(x_t, h_t, weight_ih, weight_hh, bias_hh):\n","    return torch.tanh(x_t @ weight_ih + h_t @ weight_hh + bias_hh)\n","  \n","rnn = SimpleRNN(50, 125)\n","\n","h_0 = torch.zeros(rnn.hidden_size, requires_grad=True).to(test_embeddings.device)\n","h_t = h_0\n","grads = []\n","\n","for t in range(100):\n","    h_t = rnn_step(\n","        test_embeddings[:, t, :], h_t,\n","        rnn.weight_ih, rnn.weight_hh, rnn.bias_hh,\n","    )\n","    loss = h_t.abs().sum() # we'll use the l1 norm of the current hidden state as the loss\n","    loss.backward(retain_graph=True)\n","    grads.append(torch.norm(h_0.grad).item())\n","    h_0.grad.zero_()\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X8qBPMWjARiM","colab":{}},"source":["plt.plot(grads)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eIao6JIlArk8"},"source":["As you can see, the gradients decay as time progresses. This is one of the factors that makes simple RNNs more difficult to train compared to LSTMs. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YkItayMcAtbN"},"source":["### The gradient dynamics of LSTMs\n","\n","Next, let's compare the same plot with LSTMs. Though this might not be very well known, the original formulation of the LSTM did not have a forget gate; we'll be using the formulation without the forget gate first and then see how the forget gate changes the dynamics."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pjhkaZfDA049","colab":{}},"source":["def lstm_step(x_t, h_t, c_t, W_ii, W_hi, b_i, W_if, W_hf, b_f,\n","              W_ig, W_hg, b_g, W_io, W_ho, b_o, use_forget_gate=False):\n","    i_t = torch.sigmoid(x_t @ W_ii + h_t @ W_hi + lstm.b_i)\n","    if use_forget_gate:\n","        f_t = torch.sigmoid(x_t @ W_if + h_t @ W_hf + lstm.b_f)\n","    g_t = torch.tanh(x_t @ W_ig + h_t @ W_hg + lstm.b_g)\n","    o_t = torch.sigmoid(x_t @ W_io + h_t @ W_ho + lstm.b_o)\n","    if use_forget_gate:\n","        c_t = f_t * c_t + i_t * g_t\n","    else:\n","        c_t = c_t + i_t * g_t\n","    h_t = o_t * torch.tanh(c_t)\n","    return h_t, c_t\n","  \n","lstm = NaiveLSTM(50, 125)\n","hidden_size = lstm.hidden_size\n","\n","# generate \n","h_0, c_0 = (torch.zeros(hidden_size, requires_grad=True), \n","            torch.zeros(hidden_size, requires_grad=True))\n","grads = []\n","h_t, c_t = h_0, c_0\n","for t in range(100):\n","    h_t, c_t = lstm_step(\n","        test_embeddings[:, t, :], h_t, c_t,\n","        lstm.W_ii, lstm.W_hi, lstm.b_i,\n","        lstm.W_if, lstm.W_hf, lstm.b_f,\n","        lstm.W_ig, lstm.W_hg, lstm.b_g,\n","        lstm.W_io, lstm.W_ho, lstm.b_o,\n","        use_forget_gate=False,\n","    )\n","    loss = h_t.abs().sum()\n","    loss.backward(retain_graph=True)\n","    grads.append(torch.norm(h_0.grad).item())\n","    h_0.grad.zero_()\n","    lstm.zero_grad()\n","    \n","   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8IWKA-utA83t","colab":{}},"source":["plt.plot(grads)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gCPDr8koBOhp"},"source":["Notice how the gradient keeps on accumulating. The reason the gradient behaves this way is because of the update rule\n","$$ c_t = c_{t-1} + i_t * g_t $$\n","\n","If you're familiar with gradient calculus, you'll see that the gradients for $ c_t $ propagate straight back to the gradients for $ c_{t-1} $. Therefore, the gradient of the initial timestep keeps increasing: since $ c_0 $ influences $ c_1 $, which in turn influences $ c_2 $, and so on, the influence of the initial state never disappears.\n","\n","Of course, this can be a mixed blessing: sometimes we don't want the current timestep to influence the hidden state 200 steps into the future. Sometimes, we want to \"forget\" the information we learned earlier and overwrite it with what we have newly learned. This is where the forget gate comes into play."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VpJBrST4crII"},"source":["##Excercise\n","\n","Turn the forget gate on and evaluate the performance"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"I2TQxh3aBakW"},"source":["### Turning the forget gate on\n","\n","The forget gate was originally proposed in the paper [Learning to Forget: Continual Prediction with LSTM](https://www.semanticscholar.org/paper/Learning-to-Forget%3A-Continual-Prediction-with-LSTM-Gers-Schmidhuber/11540131eae85b2e11d53df7f1360eeb6476e7f4). Let's see how the gradients change when we turn the forget gate on. Adhering to best practices, we'll initialize the bias for the forget gate to 1.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5_BQfrb6BeT1","colab":{}},"source":["lstm.b_f.data = torch.ones_like(lstm.b_f.data)\n","\n","# generate \n","h_0, c_0 = (torch.zeros(hidden_size, requires_grad=True), \n","            torch.zeros(hidden_size, requires_grad=True))\n","grads = []\n","h_t, c_t = h_0, c_0\n","for t in range(100):\n","    h_t, c_t = lstm_step(\n","        test_embeddings[:, t, :], h_t, c_t,\n","        lstm.W_ii, lstm.W_hi, lstm.b_i,\n","        lstm.W_if, lstm.W_hf, lstm.b_f,\n","        lstm.W_ig, lstm.W_hg, lstm.b_g,\n","        lstm.W_io, lstm.W_ho, lstm.b_o,\n","        use_forget_gate=True,\n","    )\n","    loss = h_t.abs().sum()\n","    loss.backward(retain_graph=True)\n","    grads.append(torch.norm(h_0.grad).item())\n","    h_0.grad.zero_()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kbLioKv9Bi3n","colab":{}},"source":["plt.plot(grads)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WU_PKZyHBsje"},"source":["Notice how the gradients decay much more slowly than in the case of the Simple RNN. On the other hand, when we don't initialize the forget gate bias to 1... "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RlxQJRZTBvAE","colab":{}},"source":["lstm.b_f.data = torch.zeros_like(lstm.b_f.data)\n","\n","h_0, c_0 = (torch.zeros(hidden_size, requires_grad=True), \n","            torch.zeros(hidden_size, requires_grad=True))\n","grads = []\n","h_t, c_t = h_0, c_0\n","for t in range(100):\n","    h_t, c_t = lstm_step(\n","        test_embeddings[:, t, :], h_t, c_t,\n","        lstm.W_ii, lstm.W_hi, lstm.b_i,\n","        lstm.W_if, lstm.W_hf, lstm.b_f,\n","        lstm.W_ig, lstm.W_hg, lstm.b_g,\n","        lstm.W_io, lstm.W_ho, lstm.b_o,\n","        use_forget_gate=True,\n","    )\n","    loss = h_t.abs().sum()\n","    loss.backward(retain_graph=True)\n","    grads.append(torch.norm(h_0.grad).item())\n","    h_0.grad.zero_()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"O1ie8RE3ByZ0","colab":{}},"source":["plt.plot(grads)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J25aFQ-tB4IS"},"source":["The gradient decays much more quickly now: this is why initializing the forget gate to 1 is a good idea, at least in the initial stages of training. \n","\n","Now, let's see what happens when we initalize the forget gate to -1.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4fFYZeTHB3OE","colab":{}},"source":["lstm.b_f.data = -torch.ones_like(lstm.b_f.data)\n","\n","# generate \n","h_0, c_0 = (torch.zeros(hidden_size, requires_grad=True), \n","            torch.zeros(hidden_size, requires_grad=True))\n","grads = []\n","h_t, c_t = h_0, c_0\n","for t in range(100):\n","    h_t, c_t = lstm_step(\n","        test_embeddings[:, t, :], h_t, c_t,\n","        lstm.W_ii, lstm.W_hi, lstm.b_i,\n","        lstm.W_if, lstm.W_hf, lstm.b_f,\n","        lstm.W_ig, lstm.W_hg, lstm.b_g,\n","        lstm.W_io, lstm.W_ho, lstm.b_o,\n","        use_forget_gate=True,\n","    )\n","    loss = h_t.abs().sum()\n","    loss.backward(retain_graph=True)\n","    grads.append(torch.norm(h_0.grad).item())\n","    h_0.grad.zero_()\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7vxf9uzrB-bX","colab":{}},"source":["plt.plot(grads)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"isKogwAPCB3P"},"source":["The weights decay even faster now.\n","\n","We looked at a lot of charts, but the most important point is that the LSTM basically has control over how much of the gradient to allow to flow through each timestep. This is what makes them so easy to train."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vaOVXUWS9gSm"},"source":["\n","\n"]}]}